{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a327ef",
   "metadata": {},
   "source": [
    "# WITHOUT ATTENTION (Note Book #1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b04037",
   "metadata": {},
   "source": [
    "---\n",
    "### Declaration\n",
    "\n",
    "In this project, I've utilized various online resources to deepen my understanding of important concepts, refine my code structure, and appropriately recognize these resources.\n",
    "\n",
    "### Significant resources that have influenced my assignment greneral DL :\n",
    "\n",
    "- **Lecture Slides:** \n",
    "  - Prof. Mitesh Khapra's course CS6910 - Fundamentals of Deep Learning.  \n",
    "    [CS6910 - Fundamentals of Deep Learning](http://www.cse.iitm.ac.in/~miteshk/CS6910.html)\n",
    "- **YouTube Lectures:** \n",
    "  - Prof. Mitesh Khapra's DeepLearning course lectures on deep learning fundamentals.  \n",
    "    [DeepLearning course lectures](https://www.youtube.com/playlist?list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT)\n",
    "- **Official Documentation:** \n",
    "  - Python NumPy and wandb.ai.  \n",
    "    [Python Documentation](https://docs.python.org/3/)  \n",
    "    [NumPy Documentation](https://numpy.org/doc/)  \n",
    "    [wandb.ai Documentation](https://docs.wandb.ai/tutorials)\n",
    "- **GitHub Repositories:** \n",
    "  - Open-source code repositories relevant to deep learning and neural networks.  \n",
    "    [Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)  \n",
    "    [Awesome Artificial Intelligence](https://github.com/owainlewis/awesome-artificial-intelligence)\n",
    "- **Academic Research:** \n",
    "  - Papers from arXiv/academic journals providing theoretical insights and recent advancements in deep learning.\n",
    "    1. **\"Adam: A Method for Stochastic Optimization\" by Kingma and Ba (2014)**  \n",
    "       [arXiv:1412.6980](https://arxiv.org/abs/1412.6980)\n",
    "    2. **\"On the Convergence of Adam and Beyond\" by Reddi, Kale, and Kumar (2018)**  \n",
    "       [arXiv:1904.09237](https://arxiv.org/abs/1904.09237)\n",
    "    3. **\"Averaged Stochastic Gradient Descent with Weight Dropped Convergence Rate\" by Junchi Li, Fadime Sener, and Vladlen Koltun (2021)**  \n",
    "       [arXiv:2106.01409](https://arxiv.org/abs/2106.01409)\n",
    "- **Online Forums:** \n",
    "  - Reddit's r/MachineLearning and r/deeplearning for discussions and knowledge sharing.  \n",
    "    [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)  \n",
    "    [r/deeplearning](https://www.reddit.com/r/deeplearning/)\n",
    "- **Coursera Courses:** \n",
    "  - Andrew Ng's ML Specialization and DL Specialization on Coursera.  \n",
    "    [Machine Learning Specialization](https://www.deeplearning.ai/courses/machine-learning-specialization/)  \n",
    "    [Deep Learning Specialization](https://www.deeplearning.ai/courses/deep-learning-specialization/)\n",
    "- **Additional Resources:** \n",
    "  - [Optimization in Deep Learning: AdaGrad, RMSProp, Adam](https://artemoppermann.com/optimization-in-deep-learning-adagrad-rmsprop-adam/)\n",
    "  - [Difference between RMSprop with momentum and Adam optimizers](https://datascience.stackexchange.com/questions/26792/difference-between-rmsprop-with-momentum-and-adam-optimizers)\n",
    "  - [Optimization Techniques in Deep Learning](https://blogs.brain-mentors.com/optimization-techniques-in-deep-learning/)\n",
    "  - [An overview of gradient descent optimization algorithms by Sebastian Ruder](https://www.ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "### Additionally, I've referred to the following resources for This specific topics:\n",
    "\n",
    "\n",
    "- **Understanding LSTM Networks by Christopher Olah (blog post):**\n",
    "  - A clear and intuitive explanation of Long Short-Term Memory (LSTM) networks, with insightful visualizations.  \n",
    "    [Link](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "- **Attention Is All You Need by Vaswani et al. (research paper):**\n",
    "  - This paper introduced the Transformer architecture, which revolutionized NLP by relying solely on attention mechanisms.  \n",
    "    [Link](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "- **Neural Machine Translation by Jointly Learning to Align and Translate by Bahdanau, Cho, and Bengio (research paper):**\n",
    "  - Proposed the first widely-used attention mechanism for neural machine translation, significantly improving performance over previous approaches.  \n",
    "    [Link](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "- **The Illustrated Transformer by Jay Alammar (blog post):**\n",
    "  - A visually engaging and easy-to-follow guide to the Transformer architecture, explaining its components and how they work together.  \n",
    "    [Link](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "- **Deep Learning for Natural Language Processing (Course 5: Sequence Models) by Andrew Ng (Coursera):**\n",
    "  - Course materials from Andrew Ng's Sequence Models course on Coursera, providing foundational knowledge in deep learning.  \n",
    "    [Coursera](https://www.coursera.org/learn/nlp-sequence-models)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5a1db",
   "metadata": {},
   "source": [
    "### Imprort library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516ac509",
   "metadata": {},
   "outputs": [],
   "source": [
    "####importing necessary packages#######\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a3747",
   "metadata": {},
   "source": [
    "### Installing Wandb\n",
    "\n",
    "- Wandb is used to keep track of various experiments performed and for efficient logging while doing hyperparameter tuning. \n",
    "- The report for this project is also created using wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b259f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahmecse\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ge22m009/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ge22m009/DL7/wandb/run-20240517_200538-iixoxuc0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmecse/Without%20attention/runs/iixoxuc0' target=\"_blank\">tough-pyramid-22</a></strong> to <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmecse/Without%20attention/runs/iixoxuc0' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/iixoxuc0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ahmecse/Without%20attention/runs/iixoxuc0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff35bec4860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a new Weights & Biases run with the specified project name\n",
    "project_name='Without attention'\n",
    "wandb.login(key=\"116333c8f36584386af5e16706d08ce3fc4d59df\")\n",
    "wandb.init(project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c096cd7",
   "metadata": {},
   "source": [
    "# Training and Validation Data Preparation\n",
    "\n",
    "**References**:\n",
    "\n",
    "1. [ documentation on LSTM Seq2Seq](https://.io/examples/nlp/lstm_seq2seq/)\n",
    "2. [Machine Learning Mastery: Define Encoder-Decoder Sequence-to-Sequence Model for Neural Machine Translation in Keras](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-/)\n",
    "3. [Stack Overflow: How can I prepare data for a Seq2Seq model?](https://stackoverflow.com/questions/59035736/how-can-i-do-prepare-data-for-a-seq2seq-model)\n",
    "4. [Analytics Vidhya: A Simple Introduction to Sequence-to-Sequence Models](https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/)\n",
    "5. [Towards Data Science: How to Implement Seq2Seq LSTM Model in ](https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in--shortcutnlp-6f355f3e5639)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4206131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/home/ge22m009/Data/aksharantar_sampled/mal/mal_train.csv', header=None, names=['English', 'Malayalam'])\n",
    "val_data = pd.read_csv('/home/ge22m009/Data/aksharantar_sampled/mal/mal_valid.csv', header=None, names=['English', 'Malayalam'])\n",
    "test_data = pd.read_csv('/home/ge22m009/Data/aksharantar_sampled/mal/mal_test.csv', header=None, names=['English', 'Malayalam'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ccf8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train = train_data['English']  # Extract English training data\n",
    "mal_train = train_data['Malayalam']  # Extract Malayalam training data\n",
    "eng_val = val_data['English']  # Extract English validation data\n",
    "mal_val = val_data['Malayalam']  # Extract Malayalam validation data\n",
    "eng_test = test_data['English']  # Extract English test data\n",
    "mal_test = test_data['Malayalam']  # Extract Malayalam test data\n",
    "english_all = pd.concat([eng_train, eng_val, eng_test], ignore_index=True)  # Concatenate all English data\n",
    "malayalam_all = pd.concat([mal_train, mal_val, mal_test], ignore_index=True)  # Concatenate all Malayalam data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d289a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_char(lan):\n",
    "    unique_cha = ''  # Initialize an empty string to store unique characters\n",
    "    for word in lan:  # Iterate through each word in the list\n",
    "        for char in word:  # Iterate through each character in the word\n",
    "            if char not in unique_cha:  # Check if the character is not already in the unique_cha string\n",
    "                unique_cha += char  # Append the unique character to the string\n",
    "    return unique_cha  # Return the string containing all unique characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53f7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LANG:\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang  # Initialize the language data\n",
    "        self.word2index = {}  # Dictionary to map characters to indices\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}  # Dictionary to map indices to characters\n",
    "        self.max_length = 0  # Variable to store the length of the longest word\n",
    "        self.count = 2  # Counter for indexing, starting after SOS and EOS\n",
    "        self.max_word = ''  # Variable to store the longest word\n",
    "\n",
    "    def addchar(self):\n",
    "        for word in self.lang:  # Iterate through each word in the language data\n",
    "            length = len(word)  # Get the length of the current word\n",
    "            if length > self.max_length:  # Check if the current word is the longest so far\n",
    "                self.max_length = length  # Update the maximum word length\n",
    "                self.max_word = word  # Update the longest word\n",
    "            for char in word:  # Iterate through each character in the word\n",
    "                if char not in self.word2index:  # Check if the character is not already indexed\n",
    "                    self.word2index[char] = self.count  # Assign the current count as the index\n",
    "                    self.index2word[self.count] = char  # Map the current count to the character\n",
    "                    self.count += 1  # Increment the counter\n",
    "        return self.word2index, self.index2word, self.max_length, self.max_word  # Return the mappings and max values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb312df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize language objects for English and Malayalam\n",
    "eng = LANG(english_all)\n",
    "# Get mappings and max values for English\n",
    "eng_word2index, eng_index2word, eng_maxlength, eng_word = eng.addchar()\n",
    "access_eng = eng_word2index, eng_index2word, eng_maxlength, eng_word\n",
    "\n",
    "mal = LANG(malayalam_all)\n",
    "# Get mappings and max values for Malayalam\n",
    "mal_word2index, mal_index2word, mal_maxlength, mal_word = mal.addchar()\n",
    "access_mal = mal_word2index, mal_index2word, mal_maxlength, mal_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e970d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensorpair(eng, mal, access_eng, access_mal):\n",
    "    # Unpack access_eng and access_mal\n",
    "    eng_word2index, eng_index2word, eng_maxlength, eng_word = access_eng\n",
    "    mal_word2index, mal_index2word, mal_maxlength, mal_word = access_mal\n",
    "    \n",
    "    n = len(eng)\n",
    "    # Initialize tensors for input and target sequences\n",
    "    input_ids = torch.zeros((n, eng_maxlength + 1), dtype=torch.int32)\n",
    "    target_ids = torch.zeros((n, eng_maxlength + 1), dtype=torch.int32)\n",
    "    \n",
    "    for i, (eng_word, mal_word) in enumerate(zip(eng, mal)):\n",
    "        try:\n",
    "            # Convert characters to indices using word2index mappings\n",
    "            input_indx = [eng_word2index[char] for char in eng_word]\n",
    "            input_indx.append(1)  # Append EOS token index\n",
    "            input_indx = torch.tensor(input_indx, dtype=torch.long)\n",
    "            \n",
    "            target_indx = [mal_word2index[char] for char in mal_word]\n",
    "            target_indx.append(1)  # Append EOS token index\n",
    "            target_indx = torch.tensor(target_indx, dtype=torch.long)\n",
    "            \n",
    "            # Update input and target tensors\n",
    "            input_ids[i, :len(input_indx)] = input_indx\n",
    "            target_ids[i, :len(target_indx)] = target_indx\n",
    "        except Exception as e:\n",
    "            print(e)  # Print any exception that occurs\n",
    "    \n",
    "    # Create a TensorDataset from input and target tensors\n",
    "    tensor_data = TensorDataset(input_ids, target_ids)\n",
    "    \n",
    "    return tensor_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40176d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating tensor pairs for training, validation, and testing data\n",
    "train_data=Tensorpair(eng_train,mal_train,access_eng,access_mal)\n",
    "val_data=Tensorpair(eng_val,mal_val,access_eng,access_mal)\n",
    "test_data=Tensorpair(eng_test,mal_test,access_eng,access_mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec3a7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting batch size and creating data loaders for training, validation, and testing\n",
    "batch_size=64\n",
    "train_loader=DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "val_loader=DataLoader(dataset=val_data,batch_size=batch_size,drop_last=True)\n",
    "test_loader=DataLoader(dataset=test_data,batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05aaf49",
   "metadata": {},
   "source": [
    "# Cell functions\n",
    "\n",
    "**References**\n",
    "\n",
    "1. [LSTM Seq2Seq Example](https://keras.io/examples/nlp/lstm_seq2seq/)\n",
    "2. [Define Encoder-Decoder Sequence-to-Sequence Model for Neural Machine Translation in ](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)\n",
    "3. [How to Implement Seq2Seq LSTM Model in ](https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639)\n",
    "4. [A Practical Guide to RNN and LSTM in ](https://towardsdatascience.com/a-practical-guide-to-rnn-and-lstm-in-keras-980f176271bc)\n",
    "5. [LSTM Layer Explained for Beginners with Example](https://machinelearningknowledge.ai/keras-lstm-layer-explained-for-beginners-with-example/)\n",
    "\n",
    "For Attention:\n",
    "\n",
    "1. [Attention Layers Documentation](https://keras.io/api/layers/attention_layers/)\n",
    "2. [A Beginner's Guide to Using Attention Layer in Neural Networks](https://analyticsindiamag.com/a-beginners-guide-to-using-attention-layer-in-neural-networks/)\n",
    "3. [Google Colab Notebook: Neural Machine Translation with Attention](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb?hl=fi)\n",
    "4. [Kaggle Discussion: Attention Mechanism](https://www.kaggle.com/questions-and-answers/279309)\n",
    "5. [LSTM Layer Explained for Beginners with Example](https://machinelearningknowledge.ai/keras-lstm-layer-explained-for-beginners-with-example/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b28b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size=512, num_layers=2, drop_out=0.2, embedding_size=256, bidirection=False, model='LSTM'):\n",
    "        super().__init__()\n",
    "        # Initialize the embedding layer\n",
    "        self.embedding = nn.Embedding(len(eng_index2word), embedding_size)\n",
    "        # Define the parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_out = drop_out\n",
    "        # Choose the RNN model (LSTM/GRU/RNN)\n",
    "        if model == 'LSTM':\n",
    "            self.model = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=bidirection, batch_first=True)\n",
    "        elif model == 'GRU':\n",
    "            self.model = nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=bidirection, batch_first=True)\n",
    "        elif model == 'RNN':\n",
    "            self.model = nn.RNN(embedding_size, hidden_size, num_layers, bidirectional=bidirection, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError('Given model is not found')\n",
    "        # Apply dropout regularization\n",
    "        self.drop_out = nn.Dropout(p=drop_out)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Embed the input sequence\n",
    "        embedded = self.drop_out(self.embedding(input))\n",
    "        # Pass the embedded sequence through the RNN model\n",
    "        output, hidden = self.model(embedded)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b82b98",
   "metadata": {},
   "source": [
    "# Inferencing model\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. [LSTM Seq2Seq Example](https://keras.io/examples/nlp/lstm_seq2seq/)\n",
    "2. [Define Encoder-Decoder Sequence-to-Sequence Model for Neural Machine Translation  ](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)\n",
    "3. [Seq2Seq Part D: Encoder-Decoder with Teacher Forcing](https://medium.com/deep-learning-with-keras/seq2seq-part-d-encoder-decoder-with-teacher-forcing-18a3a09a096)\n",
    "4. [Kaggle Notebook: Seq2Seq RNN Models (Attention + Teacher Forcing)](https://www.kaggle.com/code/residentmario/seq-to-seq-rnn-models-attention-teacher-forcing/notebook)\n",
    "5. [What is Teacher Forcing?](https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fea006bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size=512, num_layers=2, drop_out=0.2, embedding_size=256, bidirection=False, model='LSTM'):\n",
    "        super().__init__()\n",
    "        # Initialize the embedding layer\n",
    "        self.embedding = nn.Embedding(len(mal_index2word), embedding_size)\n",
    "        # Define the parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layers\n",
    "        self.drop_out = drop_out\n",
    "        # Choose the RNN model (LSTM/GRU/RNN)\n",
    "        if model == 'LSTM':\n",
    "            self.model = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=bidirection, batch_first=True)\n",
    "        elif model == 'GRU':\n",
    "            self.model = nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=bidirection, batch_first=True)\n",
    "        elif model == 'RNN':\n",
    "            self.model = nn.RNN(embedding_size, hidden_size, num_layers, bidirectional=bidirection, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError('Given model is not found')\n",
    "        # Apply dropout regularization\n",
    "        self.drop_out = nn.Dropout(p=drop_out)\n",
    "        # Define the output layer\n",
    "        if bidirection:\n",
    "            self.out = nn.Linear(2 * hidden_size, len(mal_index2word))\n",
    "        else:\n",
    "            self.out = nn.Linear(hidden_size, len(mal_index2word))\n",
    "            \n",
    "    def forward(self, input, hidden):\n",
    "        # Embed the input sequence\n",
    "        embedded = self.drop_out(self.embedding(input))\n",
    "        # Pass the embedded sequence through the RNN model\n",
    "        output, hidden = self.model(embedded, hidden)\n",
    "        # Predict the output\n",
    "        pred = self.out(output)\n",
    "        return pred, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8018bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        # Initialize the encoder and decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, inputs, targets, teacher_force_ratio):\n",
    "        # Pass the input sequence through the encoder\n",
    "        encoder_output, encoder_hidden = self.encoder(inputs)\n",
    "        # Initialize the decoder input with zeros\n",
    "        decoder_input = torch.empty(targets.shape[0], 1, dtype=torch.long, device='cuda').fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # Initialize the output tensor\n",
    "        output = torch.zeros((targets.shape[0], targets.shape[1], len(mal_index2word)), device='cuda')\n",
    "        \n",
    "        for i in range(output.shape[1]):\n",
    "            # Pass the decoder input and hidden state through the decoder\n",
    "            pred, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            pred = torch.squeeze(pred)\n",
    "            # Store the predicted output\n",
    "            output[:, i, :] = pred\n",
    "            # Update the decoder input for the next time step\n",
    "            best_guess = torch.argmax(pred, axis=-1).view(-1, 1)\n",
    "            decoder_input = best_guess if np.random.rand() > teacher_force_ratio else targets[:, i].view(-1, 1)\n",
    "            # Keep the decoder hidden state unchanged\n",
    "            decoder_hidden = decoder_hidden\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "661258bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_finder_eng(eng_):\n",
    "    # Initialize an empty list to store English words\n",
    "    full = []\n",
    "    for eng in eng_:\n",
    "        # Find the index of the end-of-sequence token (EOS)\n",
    "        eng_eos = np.where(eng == 1)[0][0]\n",
    "        # Convert the numerical representation to an English word\n",
    "        eng_word = num_word(eng[0:eng_eos], eng_index2word)\n",
    "        full.append(eng_word)\n",
    "    return np.array(full)\n",
    "\n",
    "def word_finder_mal(mal_):\n",
    "    # Initialize an empty list to store Malayalam words\n",
    "    full = []\n",
    "    for mal in mal_:\n",
    "        # Find the index of the end-of-sequence token (EOS) if present, else use the length of the sequence\n",
    "        mal_eos = np.where(mal == 1)[0][0] if 1 in mal else len(mal)\n",
    "        # Convert the numerical representation to a Malayalam word\n",
    "        mal_word = num_word(mal[0:mal_eos], mal_index2word)\n",
    "        full.append(mal_word)\n",
    "    return np.array(full)\n",
    "\n",
    "def num_word(number, converter):\n",
    "    # Convert a sequence of numerical representations to a word using the provided converter\n",
    "    number = np.array(number)\n",
    "    word = ''.join(converter[num] for num in number)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6e8e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def word_finder_eng(eng_):\n",
    "    # Initialize an empty list to store English words\n",
    "    full = []\n",
    "    for eng in eng_:\n",
    "        # Find the index of the end-of-sequence token (EOS) if present, else use the length of the sequence\n",
    "        eng_eos_indices = np.where(eng == 1)[0]\n",
    "        eng_eos = eng_eos_indices[0] if len(eng_eos_indices) > 0 else len(eng)\n",
    "        # Convert the numerical representation to an English word\n",
    "        eng_word = num_word(eng[:eng_eos], eng_index2word)\n",
    "        full.append(eng_word)\n",
    "    return np.array(full)\n",
    "\n",
    "def word_finder_mal(mal_):\n",
    "    # Initialize an empty list to store Malayalam words\n",
    "    full = []\n",
    "    for mal in mal_:\n",
    "        # Find the index of the end-of-sequence token (EOS) if present, else use the length of the sequence\n",
    "        mal_eos_indices = np.where(mal == 1)[0]\n",
    "        mal_eos = mal_eos_indices[0] if len(mal_eos_indices) > 0 else len(mal)\n",
    "        # Convert the numerical representation to a Malayalam word\n",
    "        mal_word = num_word(mal[:mal_eos], mal_index2word)\n",
    "        full.append(mal_word)\n",
    "    return np.array(full)\n",
    "\n",
    "def num_word(number, converter):\n",
    "    # Convert a sequence of numerical representations to a word using the provided converter\n",
    "    number = np.array(number)\n",
    "    word = ''.join(converter[num] for num in number)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84aec517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=30, hidden_size=512, num_layers=3, encoder_drop_out=0.2, decoder_drop_out=0.2, embedding_size=256,\n",
    "                bidirection=False, model='LSTM', lr=1e-3, optimizer_='adam', teacher_force_ratio=0.5):\n",
    "    # Set hyperparameters\n",
    "    epochs = epochs\n",
    "    hidden_size = hidden_size\n",
    "    num_layers = num_layers\n",
    "    encoder_drop_out = encoder_drop_out\n",
    "    decoder_drop_out = decoder_drop_out\n",
    "    embedding_size = embedding_size\n",
    "    bidirection = bidirection\n",
    "    model = model\n",
    "    lr = lr\n",
    "    optimizer_ = optimizer_\n",
    "    teacher_force_ratio = teacher_force_ratio\n",
    "\n",
    "    # Initialize encoder and decoder\n",
    "    encoder = Encoder(hidden_size=hidden_size, num_layers=num_layers, drop_out=encoder_drop_out, embedding_size=embedding_size,\n",
    "                      bidirection=bidirection, model=model)\n",
    "    decoder = Decoder(hidden_size=hidden_size, num_layers=num_layers, drop_out=decoder_drop_out, embedding_size=embedding_size,\n",
    "                      bidirection=bidirection, model=model)\n",
    "    # Create seq2seq model\n",
    "    model = seq2seq(encoder, decoder)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    # Define loss function\n",
    "    lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Choose optimizer\n",
    "    if optimizer_ == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_ == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError('Optimizer not found')\n",
    "\n",
    "    # Initialize lists to store training and validation metrics\n",
    "    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "    # Main training loop\n",
    "    for i in range(epochs):\n",
    "        # Initialize lists to store current epoch metrics\n",
    "        train_loss_curr, train_acc_curr, val_loss_curr, val_acc_curr = [], [], [], []\n",
    "\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over training data\n",
    "        for eng, mal in tqdm(train_loader):\n",
    "            eng = eng.to('cuda')\n",
    "            mal = mal.to('cuda')\n",
    "\n",
    "            # Forward pass\n",
    "            pred = model(eng, mal, teacher_force_ratio=teacher_force_ratio)\n",
    "            pred_loss = pred.reshape(-1, pred.shape[2])\n",
    "            mal_loss = mal.reshape(-1,).long()\n",
    "            loss = lossfun(pred_loss, mal_loss)\n",
    "\n",
    "            # Compute and store training loss\n",
    "            train_loss_curr.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute training accuracy\n",
    "            mal_ground = word_finder_mal(mal.cpu().detach().numpy())\n",
    "            pred_acc = np.argmax(pred.cpu().detach().numpy(), axis=-1).astype(np.int64)\n",
    "            mal_predicted = word_finder_mal(pred_acc)\n",
    "            acc = np.sum(mal_ground == mal_predicted) / len(mal_ground)\n",
    "            train_acc_curr.append(acc)\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Iterate over validation data\n",
    "        for eng, mal in val_loader:\n",
    "            eng = eng.to('cuda')\n",
    "            mal = mal.to('cuda')\n",
    "\n",
    "            # Forward pass (no teacher forcing)\n",
    "            with torch.no_grad():\n",
    "                pred = model(eng, mal, teacher_force_ratio=0)\n",
    "\n",
    "            pred_loss = pred.reshape(-1, pred.shape[2])\n",
    "            mal_loss = mal.reshape(-1,).long()\n",
    "\n",
    "            # Compute and store validation loss\n",
    "            loss = lossfun(pred_loss, mal_loss)\n",
    "            val_loss_curr.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            mal_ground = word_finder_mal(mal.cpu().detach().numpy())\n",
    "            pred_acc = np.argmax(pred.cpu().detach().numpy(), axis=-1)\n",
    "            mal_predicted = word_finder_mal(pred_acc)\n",
    "            acc = np.sum(mal_ground == mal_predicted) / len(mal_ground)\n",
    "            val_acc_curr.append(acc)\n",
    "\n",
    "        # Compute average metrics for the epoch\n",
    "        train_loss.append(np.average(train_loss_curr))\n",
    "        val_loss.append(np.average(val_loss_curr))\n",
    "        train_acc.append(np.average(train_acc_curr))\n",
    "        val_acc.append(np.average(val_acc_curr))\n",
    "\n",
    "        # Log metrics using wandb\n",
    "        wandb.log({\"Train_Accuracy\": np.round(train_acc[i] * 100, 2), \"Train_Loss\": train_loss[i],\n",
    "                   \"Val_Accuracy\": np.round(val_acc[i] * 100, 2), \"Val_Loss\": val_loss[i], \"Epoch\": i + 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "631a8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for hyperparameter tuning using Bayesian optimization method\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization method\n",
    "    'metric': {\n",
    "        'name': 'Val_Accuracy',  # Metric to optimize\n",
    "        'goal': 'maximize'  # Goal: maximize the validation accuracy\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the parameters to tune\n",
    "parameters_dict = {\n",
    "    'epochs': {\n",
    "        'values': [10, 15]  # Number of epochs\n",
    "    },\n",
    "    'hidden_size': {\n",
    "        'values': [32, 64, 256, 512]  # Hidden size of the model\n",
    "    },\n",
    "    'num_layers': {\n",
    "        'values': [1, 2, 3]  # Number of layers in the encoder and decoder\n",
    "    },\n",
    "    'encoder_drop_out': {\n",
    "        'values': [0.2, 0.3, 0.5]  # Dropout probability in the encoder\n",
    "    },\n",
    "    'decoder_drop_out': {\n",
    "        'values': [0.2, 0.3, 0.5]  # Dropout probability in the decoder\n",
    "    },\n",
    "    'embedding_size': {\n",
    "        'values': [32, 64, 256, 512]  # Size of the embedding layer\n",
    "    },\n",
    "    'bidirectional': {\n",
    "        'values': [True, False]  # Whether to use bidirectional RNNs or not\n",
    "    },\n",
    "    'model': {\n",
    "        'values': ['LSTM', 'GRU', 'RNN']  # Type of RNN model\n",
    "    },\n",
    "    'lr': {\n",
    "        'values': [1e-3, 1e-4, 1e-5]  # Learning rate\n",
    "    },\n",
    "    'teacher_force_ratio': {\n",
    "        'values': [0.2, 0.3, 0.4, 0.5]  # Teacher forcing ratio during training\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']  # Optimizer to use\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict  # Add parameters to the sweep configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d25070a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init()\n",
    "    # Retrieve hyperparameters from the configuration\n",
    "    config = wandb.config\n",
    "    # Set a name for the run based on hyperparameters\n",
    "    wandb.run.name = \"hidden_\" + str(config.hidden_size) + \"_layer_\" + str(config.num_layers) + \"_embedd_\" + str(config.embedding_size) + \"_bidirect_\" + str(config.bidirectional) + \"_model_\" + str(config.model)\n",
    "    \n",
    "    # Train the model with the specified hyperparameters\n",
    "    parameters = train_model(epochs=config.epochs, hidden_size=config.hidden_size, num_layers=config.num_layers,\n",
    "                             encoder_drop_out=config.encoder_drop_out, decoder_drop_out=config.decoder_drop_out,\n",
    "                             embedding_size=config.embedding_size, bidirection=config.bidirectional, model=config.model,\n",
    "                             lr=config.lr, optimizer_=config.optimizer, teacher_force_ratio=config.teacher_force_ratio)\n",
    "    \n",
    "    # Finish the Weights and Biases run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af3622",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab7e4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init()\n",
    "# sweep_id=wandb.sweep(sweep_config,project=project_name)\n",
    "# wandb.agent(sweep_id,train)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66aaf2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:iixoxuc0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f2354ce5bc45e4b2f069254d196f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.014 MB uploaded\\r'), FloatProgress(value=0.13920434752316588, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-pyramid-22</strong> at: <a href='https://wandb.ai/ahmecse/Without%20attention/runs/iixoxuc0' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/iixoxuc0</a><br/> View project at: <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_200538-iixoxuc0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:iixoxuc0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe48b138f40041b59c82ec0fe797fcd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112677888660175, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ge22m009/DL7/wandb/run-20240517_200558-0u890206</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmecse/DL7/runs/0u890206' target=\"_blank\">zany-lake-2</a></strong> to <a href='https://wandb.ai/ahmecse/DL7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmecse/DL7' target=\"_blank\">https://wandb.ai/ahmecse/DL7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmecse/DL7/runs/0u890206' target=\"_blank\">https://wandb.ai/ahmecse/DL7/runs/0u890206</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))\n",
      "<IPython.core.display.HTML object>\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: paxk11ye with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_drop_out: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_drop_out: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_force_ratio: 0.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ge22m009/DL7/wandb/run-20240517_200624-paxk11ye</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmecse/Without%20attention/runs/paxk11ye' target=\"_blank\">blooming-sweep-4</a></strong> to <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmecse/Without%20attention/runs/paxk11ye' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/paxk11ye</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70c55edf65d4bc989a3f84677aca1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23507490051e43ffabebf6ec28a31a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1b524cdd8040a9b40b25eb748555dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ae8219d3544e28b394fef345699a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58110e53134448d9ca7ba1fcdb2f0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a095d8b42b4c4033bb66bfa9a1f07f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899de026c25a434eaa86ed8f5f4e4571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11705462db824d9794ed1e76a6839847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f8e8942be1469ca8f6d47c3e4a0b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3cdd4d533b447ea2e7b58e9a14dfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b8c5a6cbb74787a6d0955655c789fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6052774a214159944dc7980c8cae86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548ad0d0dd2a4c9192f804eff3bfbe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7eb53ee3d54d829066380403533da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39111fef2da46a1b9da3f58d4f3e4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15544a3cda4489fb6a37ced3aaadb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.006 MB uploaded\\r'), FloatProgress(value=0.29687964338781575, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Train_Accuracy</td><td>▁▁▁▂▃▃▄▅▅▆▆▇▇██</td></tr><tr><td>Train_Loss</td><td>█▅▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Val_Accuracy</td><td>▁▂▃▄▅▆▆▆▇▇▇████</td></tr><tr><td>Val_Loss</td><td>█▆▄▃▃▂▂▂▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>15</td></tr><tr><td>Train_Accuracy</td><td>41.6</td></tr><tr><td>Train_Loss</td><td>0.19656</td></tr><tr><td>Val_Accuracy</td><td>35.84</td></tr><tr><td>Val_Loss</td><td>0.43544</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-sweep-4</strong> at: <a href='https://wandb.ai/ahmecse/Without%20attention/runs/paxk11ye' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/paxk11ye</a><br/> View project at: <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_200624-paxk11ye/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0bhb7mvj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_drop_out: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_drop_out: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_force_ratio: 0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ge22m009/DL7/wandb/run-20240517_205038-0bhb7mvj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmecse/Without%20attention/runs/0bhb7mvj' target=\"_blank\">avid-sweep-5</a></strong> to <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmecse/Without%20attention/runs/0bhb7mvj' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/0bhb7mvj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc457e0fc62455c8ff461010dad0754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02058a8c32314b939bbac447fcff2dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc373c1bb494fcd98aa371d609bf299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a684adfbdc44155aec0f5e8e417d586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f43afe700749e98668ce7195866e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a8d2d576df441d972c838e6ba447b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fc5a88c0c4472486a7e064253b0a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b54b3d8c1949fba6f2e9958f5440a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608f58c986bf49c4bade4bb9409c32f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c32acd56aad4b93b5a2da91f3334a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c163f8c2301a4cd79243f819749d1e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1432500de5854898a3fb20c59095aa14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae3e43bfef24d2882f5d9ffdad25759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dae9821f7e43af8cd6e50f6b357c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc40b6071314a33ac66c45aae411057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fed85f08884a74b12398009ed13724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Train_Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train_Loss</td><td>██▇▆▅▄▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Val_Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val_Loss</td><td>█▇▇▆▅▄▃▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>15</td></tr><tr><td>Train_Accuracy</td><td>0.0</td></tr><tr><td>Train_Loss</td><td>2.50478</td></tr><tr><td>Val_Accuracy</td><td>0.0</td></tr><tr><td>Val_Loss</td><td>2.01726</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-sweep-5</strong> at: <a href='https://wandb.ai/ahmecse/Without%20attention/runs/0bhb7mvj' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/0bhb7mvj</a><br/> View project at: <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_205038-0bhb7mvj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1ot1zlrj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_drop_out: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_drop_out: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_force_ratio: 0.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ge22m009/DL7/wandb/run-20240517_211047-1ot1zlrj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmecse/Without%20attention/runs/1ot1zlrj' target=\"_blank\">lucky-sweep-6</a></strong> to <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmecse/Without%20attention/runs/1ot1zlrj' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/1ot1zlrj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633b227825d341eab4053aebc03f94b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b871f0f1794940bfce5627bb41fd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64aba6f522441819e20ad019c609877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4edfaf6827c4914a1cfbf78dc7206b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7287ae2ac3104de8aa2d1199a1fdf077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34c2ae1299e463095fe7f6acdeb2f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e3b46aa43f432cbc8db1304ba6dab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfd3ab509584bb79726c7db22ca7218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972c39001b7343a8ab624832f58083f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86055effb7643d8bdf594a78486471e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6723805e3ecc4d049698c064f4959ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d9d636615649e497fcbd027b1dc0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108391f369b44df9b79cdf27cbec9dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5ec990dbc44b21af2d22d6e7c83c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce87b59ab2c4a3a96c9d1df4ee085f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e0442efce74a7fa1e757c6d4b18569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Train_Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train_Loss</td><td>█▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Val_Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val_Loss</td><td>█▄▃▂▂▂▂▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>15</td></tr><tr><td>Train_Accuracy</td><td>0.0</td></tr><tr><td>Train_Loss</td><td>1.33172</td></tr><tr><td>Val_Accuracy</td><td>0.0</td></tr><tr><td>Val_Loss</td><td>1.12017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-sweep-6</strong> at: <a href='https://wandb.ai/ahmecse/Without%20attention/runs/1ot1zlrj' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/1ot1zlrj</a><br/> View project at: <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_211047-1ot1zlrj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: avieagqb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_drop_out: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_drop_out: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_force_ratio: 0.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ge22m009/DL7/wandb/run-20240517_212900-avieagqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmecse/Without%20attention/runs/avieagqb' target=\"_blank\">spring-sweep-7</a></strong> to <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/sweeps/pxmwdyhc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmecse/Without%20attention/runs/avieagqb' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/avieagqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef730f965d34e02b07bd9caa515180f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4dd70985e19498b85e8a4de809cd697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df70791a4034b1ea5bbacd570ec95fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016dea34d1ee4a5aa9770bc3a095ade5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6d65a68edc4c6088b1c9afc7829679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d391de7b97c4d0ea66673dac4ab1d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134ce295745b48d59e5ff47b4c459ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d37dddfba084a4c8e2b024a62da576d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baa39c23c5448cb94c0b69773e7e86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07dff7c830e74615828b6f64c3b1de29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b69dfc7cb6f40ff827177f67a5dc5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.014 MB uploaded\\r'), FloatProgress(value=0.13306236673773988, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train_Accuracy</td><td>▁▁▁▁▁▁▁▃▅█</td></tr><tr><td>Train_Loss</td><td>█▆▅▅▄▃▂▂▁▁</td></tr><tr><td>Val_Accuracy</td><td>▁▁▁▁▁▂▃▄▅█</td></tr><tr><td>Val_Loss</td><td>█▇▆▅▄▄▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train_Accuracy</td><td>0.88</td></tr><tr><td>Train_Loss</td><td>0.68474</td></tr><tr><td>Val_Accuracy</td><td>4.44</td></tr><tr><td>Val_Loss</td><td>0.70101</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-sweep-7</strong> at: <a href='https://wandb.ai/ahmecse/Without%20attention/runs/avieagqb' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention/runs/avieagqb</a><br/> View project at: <a href='https://wandb.ai/ahmecse/Without%20attention' target=\"_blank\">https://wandb.ai/ahmecse/Without%20attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_212900-avieagqb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "wandb.init()\n",
    "sweep_id = 'pxmwdyhc'  # Replace with your actual sweep ID\n",
    "wandb.agent(sweep_id, function=train, project=project_name)  \n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "703e9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count=0\n",
    "for eng,mal in train_data:\n",
    "    eng_word=word_finder_eng(eng)\n",
    "    mal_word=word_finder_mal(mal)\n",
    "    print(eng_word)\n",
    "    print(mal_word)\n",
    "    eng=eng.unsqueeze(0).to('cuda')\n",
    "    mal=mal.unsqueeze(0).to('cuda')\n",
    "    \n",
    "    pred=model(eng,mal,teacher_force_ratio=0)\n",
    "    pred=torch.squeeze(pred).cpu().detach().numpy()\n",
    "    pred=np.argmax(pred,axis=-1)\n",
    "    predicted=word_finder_mal(pred)\n",
    "    if predicted==mal_word:\n",
    "        correct_count+=1\n",
    "    count+=1\n",
    "    print(predicted)\n",
    "    print(\"\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "545caf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_finder_eng(eng):\n",
    "    eng_eos=np.where(eng==1)[0][0]\n",
    "    eng_word=num_word(eng[0:eng_eos],eng_index2word)\n",
    "    return eng_word\n",
    "def word_finder_mal(mal):\n",
    "    mal_eos=np.where(mal==1)[0][0]\n",
    "    mal_word=num_word(mal[0:mal_eos],mal_index2word)\n",
    "    return mal_word\n",
    "def num_word(number,converter):\n",
    "    number=np.array(number)\n",
    "    word=''.join(converter[num]for num in number)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b10f17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(epochs=15,hidden_size=512,num_layers=2,encoder_drop_out=0.5,decoder_drop_out=0.2,embedding_size=32,\n",
    "    bidirection=True,model='LSTM',lr=0.001,optimizer_='adam',teacher_force_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d27ab735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=30,hidden_size=512,num_layers=3,encoder_drop_out=0.2,decoder_drop_out=0.2,embedding_size=256,\n",
    "                bidirection=False,model='LSTM',lr=1e-3,optimizer_='adam',teacher_force_ratio=0.5):\n",
    "    epochs=epochs\n",
    "    hidden_size=hidden_size\n",
    "    num_layers=num_layers\n",
    "    encoder_drop_out=encoder_drop_out\n",
    "    decoder_drop_out=decoder_drop_out\n",
    "    embedding_size=embedding_size\n",
    "    bidirection=bidirection\n",
    "    model=model\n",
    "    lr=lr\n",
    "    optimizer_=optimizer_\n",
    "    teacher_force_ratio=teacher_force_ratio\n",
    "\n",
    "    encoder=Encoder(hidden_size=hidden_size,num_layers=num_layers,drop_out=encoder_drop_out,embedding_size=embedding_size,\n",
    "                    bidirection=bidirection,model=model)\n",
    "    decoder=Decoder(hidden_size=hidden_size,num_layers=num_layers,drop_out=decoder_drop_out,embedding_size=embedding_size,\n",
    "                    bidirection=bidirection,model=model)\n",
    "    model=seq2seq(encoder,decoder)\n",
    "    model=model.to('cuda')\n",
    "    lossfun=nn.CrossEntropyLoss()\n",
    "    if optimizer_=='adam':\n",
    "        optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    elif optimizer_=='sgd':\n",
    "        optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
    "    else:\n",
    "        raise ValueError('Optimizer not found')\n",
    "    train_loss,train_acc,val_loss,val_acc=[],[],[],[]\n",
    "    curr_best_acc=0\n",
    "    for i in range(epochs):\n",
    "        train_loss_curr,train_acc_curr,val_loss_curr,val_acc_curr=[],[],[],[]\n",
    "        model.train()\n",
    "        for eng,mal in tqdm(train_loader):\n",
    "            eng=eng.to('cuda')\n",
    "            mal=mal.to('cuda')\n",
    "            pred=model(eng,mal,teacher_force_ratio=teacher_force_ratio)\n",
    "            #pred=nn.Softmax(dim=-1)(pred)\n",
    "            pred_loss=pred.reshape(-1,pred.shape[2])\n",
    "            mal_loss=mal.reshape(-1,).long()\n",
    "            loss=lossfun(pred_loss,mal_loss)\n",
    "\n",
    "            train_loss_curr.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            mal_ground=word_finder_mal(mal.cpu().detach().numpy())\n",
    "            pred_acc=np.argmax(pred.cpu().detach().numpy(),axis=-1).astype(np.int64)\n",
    "            mal_predicted=word_finder_mal(pred_acc)\n",
    "            acc=np.sum(mal_ground==mal_predicted)/len(mal_ground)\n",
    "            train_acc_curr.append(acc)\n",
    "        model.eval()\n",
    "        for eng,mal in test_loader:\n",
    "            eng=eng.to('cuda')\n",
    "            mal=mal.to('cuda')\n",
    "            with torch.no_grad():\n",
    "                pred=model(eng,mal,teacher_force_ratio=0)\n",
    "            #pred=nn.Softmax(dim=-1)(pred)\n",
    "            pred_loss=pred.reshape(-1,pred.shape[2])\n",
    "            mal_loss=mal.reshape(-1,).long()\n",
    "\n",
    "            loss=lossfun(pred_loss,mal_loss)\n",
    "            val_loss_curr.append(loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "            mal_ground=word_finder_mal(mal.cpu().detach().numpy())\n",
    "            pred_acc=np.argmax(pred.cpu().detach().numpy(),axis=-1)\n",
    "\n",
    "            mal_predicted=word_finder_mal(pred_acc)\n",
    "            acc=np.sum(mal_ground==mal_predicted)/len(mal_ground)\n",
    "            val_acc_curr.append(acc)\n",
    "        train_loss.append(np.average(train_loss_curr))\n",
    "        val_loss.append(np.average(val_loss_curr))\n",
    "        train_acc.append(np.average(train_acc_curr))\n",
    "        val_acc.append(np.average(val_acc_curr))\n",
    "        if val_acc[i]>curr_best_acc:\n",
    "            torch.save(model.state_dict(),'weight_without_attention.pth')\n",
    "            curr_best_acc=val_acc[i]\n",
    "            print(f'epochs{i} weights saved')\n",
    "        print(f'Epochs {i} completed, train loss and accuracy ={train_loss[i],train_acc[i]}' \n",
    "          f',and val loss and accuracy ={val_loss[i],val_acc[i]} ')\n",
    "        '''\n",
    "        wandb.log({\"Train_Accuracy\":np.round(train_acc[i]*100,2),\"Train_Loss\":train_loss[i],\n",
    "                   \"Val_Accuracy\":np.round(val_acc[i]*100,2),\"Val_Loss\":val_loss[i],\"Epoch\":i+1})\n",
    "        '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb18880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
